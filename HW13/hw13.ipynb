{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darkqq/miniconda3/envs/ML2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\n",
    "from torchvision.datasets import DatasetFolder, VisionDataset\n",
    "from torchsummary import summary\n",
    "from models.student import *\n",
    "\n",
    "# This is for the progress bar.\n",
    "from tqdm.auto import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'dataset_root': './Food-11',\n",
    "    'save_dir': './outputs',\n",
    "    'exp_name': \"distill_small_10000\",\n",
    "    'batch_size': 64,\n",
    "    'lr': 3e-4,\n",
    "    'num_workers': 4,\n",
    "    'seed': 20220013,\n",
    "    'loss_fn_type': 'CE', # simple baseline: CE, medium baseline: KD. See the Knowledge_Distillation part for more information.\n",
    "    'weight_decay': 1e-5,\n",
    "    'grad_norm_max': 10,\n",
    "    'n_epochs': 5000, # train more steps to pass the medium baseline.\n",
    "    'patience': 10000,\n",
    "    'alpha': 0.35,\n",
    "    'beta': 0.35,\n",
    "    'temperature': 4.0,\n",
    "    'loss_fn': 'kd_with_features',\n",
    "    'dim_mapper_keys': ['pre_layer', 'layer1', 'layer2', 'layer3', 'layer4', 'avg_pooled']\n",
    "}\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_root': './Food-11', 'save_dir': './outputs', 'exp_name': 'distill_small_10000', 'batch_size': 64, 'lr': 0.0003, 'num_workers': 4, 'seed': 20220013, 'loss_fn_type': 'CE', 'weight_decay': 1e-05, 'grad_norm_max': 10, 'n_epochs': 5000, 'patience': 10000, 'alpha': 0.35, 'beta': 0.35, 'temperature': 4.0, 'loss_fn': 'kd_with_features', 'dim_mapper_keys': ['pre_layer', 'layer1', 'layer2', 'layer3', 'layer4', 'avg_pooled']}\n"
     ]
    }
   ],
   "source": [
    "myseed = cfg['seed']  # set a random seed for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "random.seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)\n",
    "\n",
    "save_path = os.path.join(cfg['save_dir'], cfg['exp_name']) # create saving directory\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# define simple logging functionality\n",
    "log_fw = open(f\"{save_path}/log.txt\", 'w') # open log file to save log outputs\n",
    "def log(text):     # define a logging function to trace the training process\n",
    "    print(text)\n",
    "    log_fw.write(str(text)+'\\n')\n",
    "    log_fw.flush()\n",
    "\n",
    "log(cfg)  # log your configs to the log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normally, We don't need augmentations in testing and validation.\n",
    "# # All we need here is to resize the PIL image and transform it into Tensor.\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "test_tfm = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "# # However, it is also possible to use augmentation in the testing phase.\n",
    "# # You may use train_tfm to produce a variety of images and then test using ensemble methods\n",
    "policy = transforms.AutoAugmentPolicy.IMAGENET\n",
    "augmenter = transforms.AutoAugment(policy)\n",
    "train_tfm = transforms.Compose([\n",
    "    # transforms.Resize(224),\n",
    "    # transforms.CenterCrop(224),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, path, tfm=test_tfm, augmenter=None, files = None):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n",
    "        if files != None:\n",
    "            self.files = files\n",
    "        print(f\"One {path} sample\",self.files[0])\n",
    "        self.transform = tfm\n",
    "        if augmenter != None:\n",
    "            print('Use augmenter.')\n",
    "        self.augmenter = augmenter\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "  \n",
    "    def __getitem__(self,idx):\n",
    "        fname = self.files[idx]\n",
    "        im = Image.open(fname)\n",
    "        if self.augmenter != None:\n",
    "            im = self.augmenter(im)\n",
    "        im = self.transform(im)\n",
    "        #im = self.data[idx]\n",
    "        try:\n",
    "            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n",
    "        except:\n",
    "            label = -1 # test has no label\n",
    "        return im,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One ./Food-11/training sample ./Food-11/training/0_0.jpg\n",
      "Use augmenter.\n",
      "One ./Food-11/validation sample ./Food-11/validation/0_0.jpg\n"
     ]
    }
   ],
   "source": [
    "train_set = FoodDataset(os.path.join(cfg['dataset_root'], \"training\"), tfm=train_tfm, augmenter=augmenter)\n",
    "train_loader = DataLoader(train_set, batch_size=cfg['batch_size'], shuffle=True, num_workers=cfg['num_workers'], pin_memory=True)\n",
    "\n",
    "valid_set = FoodDataset(os.path.join(cfg['dataset_root'], \"validation\"), tfm=test_tfm)\n",
    "valid_loader = DataLoader(valid_set, batch_size=cfg['batch_size'], shuffle=False, num_workers=cfg['num_workers'], pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "dim_mapper = {\n",
    "    'pre_layer': (32, 64),\n",
    "    'layer1': (32, 64),\n",
    "    'layer2': (64, 128),\n",
    "    'layer3': (64, 256),\n",
    "    'layer4': (64, 512),\n",
    "    'avg_pooled': (64, 512),\n",
    "    'pre_logits': (64, 512),\n",
    "}\n",
    "dim_mapper_models = {}\n",
    "for key in cfg['dim_mapper_keys']:\n",
    "    assert key in dim_mapper.keys()\n",
    "    in_ch, out_ch = dim_mapper[key]\n",
    "    if in_ch == out_ch:\n",
    "        continue\n",
    "    if key == 'pre_logits':\n",
    "        raise NotImplementedError\n",
    "    new_mapper_model = nn.Conv2d(in_ch, out_ch, 1)\n",
    "    new_mapper_model.to(device)\n",
    "    dim_mapper_models[key] = new_mapper_model\n",
    "    nn.init.kaiming_normal_(new_mapper_model.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "\n",
    "def set_dim_mapper_models_mode(mode):\n",
    "    if mode == 'train':\n",
    "        for key in cfg['dim_mapper_keys']:\n",
    "            if dim_mapper_models.get(key, None):\n",
    "                dim_mapper_models[key].train()\n",
    "    elif mode == 'eval':\n",
    "        for key in cfg['dim_mapper_keys']:\n",
    "            if dim_mapper_models.get(key, None):\n",
    "                dim_mapper_models[key].eval()\n",
    "\n",
    "writer = SummaryWriter(log_dir=f\"{save_path}/tb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = cfg['n_epochs']\n",
    "patience = cfg['patience'] # If no improvement in 'patience' epochs, early stop\n",
    "\n",
    "teacher_model = resnet18(num_classes=11, output_whole_layers=True)\n",
    "teacher_model.load_state_dict(torch.load(f\"./pretrain/best.ckpt\", map_location='cpu'))\n",
    "summary(teacher_model, (3, 224, 224), device='cpu')\n",
    "student_model = resnet_dp_small(num_classes=11, output_whole_layers=True)\n",
    "summary(student_model, (3, 224, 224), device='cpu')\n",
    "\n",
    "# # Initialize a model, and put it on the device specified.\n",
    "teacher_model.to(device)\n",
    "student_model.to(device)\n",
    "\n",
    "# # For the classification task, we use cross-entropy as the measurement of performance.\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "def loss_fn_kd(outputs, labels, teacher_outputs, alpha=cfg['alpha'], temperature=cfg['temperature']):\n",
    "    T = temperature\n",
    "    kl_loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    kl_loss = kl_loss_fn(F.log_softmax(outputs/T, dim=1), F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T)\n",
    "    return  kl_loss + F.cross_entropy(outputs, labels) * (1. - alpha)\n",
    "\n",
    "def loss_fn_l2(outputs, labels, teacher_outputs, alpha=cfg['alpha']):\n",
    "    l2_loss_fn = nn.MSELoss()\n",
    "    l2_loss = l2_loss_fn(outputs, teacher_outputs)\n",
    "    return  l2_loss * alpha + F.cross_entropy(outputs, labels) * (1. - alpha)\n",
    "\n",
    "def loss_fn_kd_with_features(outputs, labels, teacher_outputs, \n",
    "    alpha=cfg['alpha'], beta=cfg['beta'], temperature=cfg['temperature'], \n",
    "    dim_mapper_keys=cfg['dim_mapper_keys']):\n",
    "    student_logits = outputs['logits']\n",
    "    teacher_logits = teacher_outputs['logits']\n",
    "    T = temperature\n",
    "    kl_loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    kl_loss = kl_loss_fn(F.log_softmax(student_logits/T, dim=1), F.softmax(teacher_logits/T, dim=1)) * (alpha * T * T)\n",
    "    CE_loss = F.cross_entropy(student_logits, labels) * (1. - alpha - beta)\n",
    "    feats_l2_loss = 0.0\n",
    "    l2_loss_fn = nn.MSELoss()\n",
    "    for key in dim_mapper_keys:\n",
    "        student_feats = outputs[key]\n",
    "        # print(f\"{key}: {student_feats.size()}\")\n",
    "        teacher_feats = torch.flatten(teacher_outputs[key], start_dim=1)\n",
    "        if dim_mapper_models.get(key, None):\n",
    "            student_feats_teacher_dim = torch.flatten(dim_mapper_models[key](student_feats), start_dim=1)\n",
    "        else:\n",
    "            student_feats_teacher_dim = torch.flatten(student_feats, start_dim=1)\n",
    "        feats_l2_loss += l2_loss_fn(student_feats_teacher_dim, teacher_feats)\n",
    "    feats_l2_loss *= beta\n",
    "    return (kl_loss + CE_loss + feats_l2_loss, kl_loss, CE_loss, feats_l2_loss)\n",
    "    \n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "loss_fn_type = cfg['loss_fn']\n",
    "loss_fn = eval(f\"loss_fn_{loss_fn_type}\")\n",
    "\n",
    "trainable_params = []\n",
    "for key in cfg['dim_mapper_keys']:\n",
    "    mapper_model = dim_mapper_models.get(key, None)\n",
    "    if mapper_model != None:\n",
    "        trainable_params += list(mapper_model.parameters())\n",
    "trainable_params += student_model.parameters()\n",
    "# # Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.\n",
    "optimizer = torch.optim.Adam(trainable_params, lr=cfg['lr'], weight_decay=cfg['weight_decay']) \n",
    "\n",
    "# # Initialize trackers, these are not parameters and should not be changed\n",
    "stale = 0\n",
    "best_acc = 0.0\n",
    "\n",
    "teacher_model.eval()\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # ---------- Training ----------\n",
    "    # Make sure the model is in train mode before training.\n",
    "    student_model.train()\n",
    "    set_dim_mapper_models_mode('train')\n",
    "\n",
    "    # These are used to record information in training.\n",
    "    train_total_loss = []\n",
    "    train_kl_loss = []\n",
    "    train_CE_loss = []\n",
    "    train_feats_l2_loss = []\n",
    "    train_accs = []\n",
    "    train_lens = []\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "\n",
    "        # A batch consists of image data and corresponding labels.\n",
    "        imgs, labels = batch\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #imgs = imgs.half()\n",
    "        #print(imgs.shape,labels.shape)\n",
    "\n",
    "        # Forward the data. (Make sure data and model are on the same device.)\n",
    "        outputs = student_model(imgs)\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(imgs)\n",
    "\n",
    "        # Calculate the cross-entropy loss.\n",
    "        # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n",
    "        total_loss, kl_loss, CE_loss, feats_l2_loss = loss_fn(outputs, labels, teacher_outputs)\n",
    "\n",
    "        # Gradients stored in the parameters in the previous step should be cleared out first.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute the gradients for parameters.\n",
    "        total_loss.backward()\n",
    "\n",
    "        # Clip the gradient norms for stable training.\n",
    "        grad_norm = nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=cfg['grad_norm_max'])\n",
    "\n",
    "        # Update the parameters with computed gradients.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute the accuracy for current batch.\n",
    "        acc = (outputs['logits'].argmax(dim=-1) == labels).float().sum()\n",
    "\n",
    "        # Record the loss and accuracy.\n",
    "        train_batch_len = len(imgs)\n",
    "        train_total_loss.append(total_loss.item() * train_batch_len)\n",
    "        train_kl_loss.append(kl_loss.item() * train_batch_len)\n",
    "        train_CE_loss.append(CE_loss.item() * train_batch_len)\n",
    "        train_feats_l2_loss.append(feats_l2_loss.item() * train_batch_len)\n",
    "        train_accs.append(acc)\n",
    "        train_lens.append(train_batch_len)\n",
    "        \n",
    "    train_total_loss = sum(train_total_loss) / sum(train_lens)\n",
    "    train_kl_loss = sum(train_kl_loss) / sum(train_lens)\n",
    "    train_CE_loss = sum(train_CE_loss) / sum(train_lens)\n",
    "    train_feats_l2_loss = sum(train_feats_l2_loss) / sum(train_lens)\n",
    "    train_acc = sum(train_accs) / sum(train_lens)\n",
    "\n",
    "    # Print the information.\n",
    "    log(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = ({train_total_loss:.5f}, {train_kl_loss:.5f}, {train_CE_loss:.5f}, {train_feats_l2_loss:.5f}), acc = {train_acc:.5f}\")\n",
    "    writer.add_scalar('Loss_total/train', train_total_loss, epoch)\n",
    "    writer.add_scalar('Loss_KL/train', train_kl_loss, epoch)\n",
    "    writer.add_scalar('Loss_CE/train', train_CE_loss, epoch)\n",
    "    writer.add_scalar('Loss_L2_feats/train', train_feats_l2_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "    # ---------- Validation ----------\n",
    "    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n",
    "    student_model.eval()\n",
    "    set_dim_mapper_models_mode('eval')\n",
    "\n",
    "    # These are used to record information in validation.\n",
    "    valid_total_loss = []\n",
    "    valid_kl_loss = []\n",
    "    valid_CE_loss = []\n",
    "    valid_feats_l2_loss = []\n",
    "    valid_accs = []\n",
    "    valid_lens = []\n",
    "\n",
    "    # Iterate the validation set by batches.\n",
    "    for batch in tqdm(valid_loader):\n",
    "\n",
    "        # A batch consists of image data and corresponding labels.\n",
    "        imgs, labels = batch\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #imgs = imgs.half()\n",
    "\n",
    "        # We don't need gradient in validation.\n",
    "        # Using torch.no_grad() accelerates the forward process.\n",
    "        with torch.no_grad():\n",
    "            outputs = student_model(imgs)\n",
    "            teacher_outputs = teacher_model(imgs)\n",
    "\n",
    "        # We can still compute the loss (but not the gradient).\n",
    "        total_loss, kl_loss, CE_loss, feats_l2_loss = loss_fn(outputs, labels, teacher_outputs)\n",
    "\n",
    "        # Compute the accuracy for current batch.\n",
    "        acc = (outputs['logits'].argmax(dim=-1) == labels).float().sum()\n",
    "\n",
    "        # Record the loss and accuracy.\n",
    "        batch_len = len(imgs)\n",
    "        valid_total_loss.append(total_loss.item() * train_batch_len)\n",
    "        valid_kl_loss.append(kl_loss.item() * train_batch_len)\n",
    "        valid_CE_loss.append(CE_loss.item() * train_batch_len)\n",
    "        valid_feats_l2_loss.append(feats_l2_loss.item() * train_batch_len)\n",
    "        valid_accs.append(acc)\n",
    "        valid_lens.append(batch_len)\n",
    "        #break\n",
    "\n",
    "    # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
    "    valid_total_loss = sum(valid_total_loss) / sum(valid_lens)\n",
    "    valid_kl_loss = sum(valid_kl_loss) / sum(valid_lens)\n",
    "    valid_CE_loss = sum(valid_CE_loss) / sum(valid_lens)\n",
    "    valid_feats_l2_loss = sum(valid_feats_l2_loss) / sum(valid_lens)\n",
    "    valid_acc = sum(valid_accs) / sum(valid_lens)\n",
    "    writer.add_scalar('Loss_total/valid', train_total_loss, epoch)\n",
    "    writer.add_scalar('Loss_KL/valid', valid_kl_loss, epoch)\n",
    "    writer.add_scalar('Loss_CE/valid', valid_CE_loss, epoch)\n",
    "    writer.add_scalar('Loss_L2_feats/valid', valid_feats_l2_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/valid', valid_acc, epoch)\n",
    "\n",
    "    # update logs\n",
    "    \n",
    "    if valid_acc > best_acc:\n",
    "        log(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = ({valid_total_loss:.5f}, {valid_kl_loss:.5f}, {valid_CE_loss:.5f}, {valid_feats_l2_loss:.5f}), acc = {valid_acc:.5f} -> best\")\n",
    "        writer.add_scalar('BestAcc/valid', valid_acc, epoch)\n",
    "    else:\n",
    "        log(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = ({valid_total_loss:.5f}, {valid_kl_loss:.5f}, {valid_CE_loss:.5f}, {valid_feats_l2_loss:.5f}), acc = {valid_acc:.5f}\")\n",
    "\n",
    "\n",
    "    # save models\n",
    "    if valid_acc > best_acc:\n",
    "        log(f\"Best model found at epoch {epoch}, saving model\")\n",
    "        torch.save(student_model.state_dict(), f\"{save_path}/student_best.ckpt\") # only save best to prevent output memory exceed error\n",
    "        best_acc = valid_acc\n",
    "        stale = 0\n",
    "    else:\n",
    "        stale += 1\n",
    "        if stale > patience:\n",
    "            log(f\"No improvment {patience} consecutive epochs, early stopping\")\n",
    "            break\n",
    "    log_fw.flush()\n",
    "log(\"Finish training\")\n",
    "log_fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One ./Food-11/evaluation sample ./Food-11/evaluation/0000.jpg\n"
     ]
    }
   ],
   "source": [
    "# create dataloader for evaluation\n",
    "eval_set = FoodDataset(os.path.join(cfg['dataset_root'], \"evaluation\"), tfm=test_tfm)\n",
    "eval_loader = DataLoader(eval_set, batch_size=cfg['batch_size'], shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:08<00:00,  4.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load model from {exp_name}/student_best.ckpt\n",
    "student_model_best = resnet_dp_small(num_classes=11, output_whole_layers=True) # get a new student model to avoid reference before assignment.\n",
    "ckpt_path = f\"{save_path}/student_best.ckpt\" # the ckpt path of the best student model.\n",
    "student_model_best.load_state_dict(torch.load(ckpt_path, map_location='cpu')) # load the state dict and set it to the student model\n",
    "student_model_best.to(device) # set the student model to device\n",
    "\n",
    "# Start evaluate\n",
    "student_model_best.eval()\n",
    "eval_preds = [] # storing predictions of the evaluation dataset\n",
    "\n",
    "# Iterate the validation set by batches.\n",
    "for batch in tqdm(eval_loader):\n",
    "    # A batch consists of image data and corresponding labels.\n",
    "    imgs, _ = batch\n",
    "    # We don't need gradient in evaluation.\n",
    "    # Using torch.no_grad() accelerates the forward process.\n",
    "    with torch.no_grad():\n",
    "        # print(logits)\n",
    "        logits = student_model_best(imgs.to(device))\n",
    "        logits = logits['logits']\n",
    "        preds = list(logits.argmax(dim=-1).squeeze().cpu().numpy())\n",
    "    # loss and acc can not be calculated because we do not have the true labels of the evaluation set.\n",
    "    eval_preds += preds\n",
    "\n",
    "def pad4(i):\n",
    "    return \"0\"*(4-len(str(i))) + str(i)\n",
    "\n",
    "# Save prediction results\n",
    "ids = [pad4(i) for i in range(0,len(eval_set))]\n",
    "categories = eval_preds\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Id'] = ids\n",
    "df['Category'] = categories\n",
    "df.to_csv(f\"{save_path}/submission.csv\", index=False) # now you can download the submission.csv and upload it to the kaggle competition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
